## 여성 홈쇼핑 데이터 분석

![bandicam 2021-11-08 14-45-44-074](https://user-images.githubusercontent.com/66814045/140690430-a28e0e7b-2b13-4c52-bbdb-405cee447d23.jpg)


파일 설명 
- 프로젝트 메인파일 : shopping_model_.ver2(log).ipynb (메인 코드)


### 💻프로젝트를 시작 하게 된 이유
E-commerce에 관심이 많아서 관련데이터를 찾다가 다양한 인사이트와 개개인의 분석방법에 따른 전략을 이끌어낼 수 있는 **가장 자율성이 높은** 데이터라고 생각했기 때문에 선택하게 되었습니다.   


### 💻프로젝트 목적
쇼핑몰에 입점해 있는 고객들을 여러목적에 따라 분류 하는 연습을 해보고 실제 마케팅 혹은 기획전략팀에서 프로젝트를 하는데 있어서 필요할 데이터와 인사이틀 전달해주는것이 주 목적이며,
모델링을 통하여 고객들의 상품정보를 통해 판매 예측을 하는 시스템을 만들어 자가 피드백이 가능한 기술을 구현하는것.


### 💡프로젝트 진행과정 및 새롭게 배운점💡

**[ 분석 진행방법!! ]**
1. 처음에는데이터에 대한 모델링의 가능성을 파악해보자고 생각을 했기때문에 전체과정을 린하게 싸이클을 돌리고 디테일하게 분석하기로 결정을 했다. 
싸이클을 전체 돌아보고 모델링하기에 크롤링된 테이블 구성이 적합하지는 않다고 판단을 했다. 그래서, 고객segmentation(세분화) 과 마케팅,기획팀과 협업을 위한 인사이트를
제공하기 위한 목적으로 전환을 하였다.

**[ 기계적인 분석에서 능동적인 분석을 시도하다!! ]**

2. 중복데이터를 다룰때 보통 삭제를 하고 넘어가는 기계적인 방식으로 분석을 하는 교과서적이기만 했던 방식에서 좀 더 분석적으로 접근하여 상품과 판매자의 중복값을 각각발견하였고
해당 중복값이 크롤링과정의 에러에 의한 중복도 있었지만 개개인의 판매자가 여러상품을 판매함으로서 나오는 중복값이 있다는것을 발견하게되었다. 이에 중복데이터의 특성에 따라 한명의 고객이 몇개의 상품을 파는가에 따라 등급을 나눈 "상품판매수량 등급" 컬럼과 복수개(2개이상)의 상품을 판매하는 판매자의 모든상품이 1000개이상 팔렸을때 "인기판매회원" 이라는 두개의 새로운 feature를 만드는데 성공했으며 이에 따른 좀 더 세분화된 마케팅전략과 회사의 기획전략의 다양성에 도움을 줄수 있음을 기대하고있다.

**[ 데이터에 매물되지 않고 경험적 판단을 융합하다!! ]**

3. 처음 엘레베이터가 나왔을때 속도가 느리다는 불만이 있어 누군가는 속도를 줄이기 위해 고액의 전문가를 모셨고 누구는 거울을 설치했다. 데이터 분석을 하다보면 데이터와 방법론 그리고 
수치에 매몰되기 쉽다, 그래서 항상 경험적인 요소와 다각도로 생각하는것이 중요하다 처음으로 간단하고 쉬울수 있지만 이것을 적용해 보았다. 클러스터링을 통해 전체데이터에서 우리에게 수익을 많이 가져다주는 고객군을 형성하고자 했는데 Elbow method와 Silhouette score 두가지를 확인해본 결과 최적의 군집k값은 2개가 겹치는 것으로 나왔다. 평소에는 이를 그대로 받아들였지만 앞선 2번의 능동적인 분석과 비슷한 내용이지만 고객들을 극단적으로 두분류로 나누는 것보다 중간층이 있어 좀 더 각 군집층에 맞게 분석을 할 수 있도록 2개의 k값 다음으로 높았던 3개로 정하게 되었다. 

**[ 시각화를 할때 화려한 색이 아닌 전달목적의 시각화를 해보다!! ]**

4. 데이터분석을 배울때 시각화 즉, visualization은 빼먹을 수 없는 역량이다. 하지만 시각화를 배우면서 오색찬란한 색을 사용하여 이쁘게 꾸미고 바이올린plot같은 신기한 시각화를 하는경우가 많다. 나또한 최근까지 그래왔다. 하지만 이번 프로젝트에서는 어떻게하면 시각화의 본분을 잘 전달할 수 있을지 고민하고 이를 위한 커스텀 작업을 부족하지만 시도를 해보았다. 이를 통해 이쁘고 하고싶은 시각화가 아닌 요약을 하고 설명하는데 필요한 시각화 방법을 하는것의 중요성을 알게되었고, 아직 모든 시각화를 이러한 관점에서 적용을 끝마치지 못하였지만 일부난잡한 시각화를 우선순위로 보기쉽고, 이해하기 쉬운 방법으로 변경하였다. (지속적으로 업데이트 예정)





### 💻프로젝트 진행하면서 어려웠던 부분 및 배운점
- 기본 전처리 단계의 중요성 -> 중복제거와 같은 기본단계의 실수나 놓친부분이 생기게 됨에 따라 가설검정의 결과가 달라졌음.
- 내가 한 가설검정의 방법이 적재적소에 맞게 쓴것인가?, 혹은 단순 시각화 만으로 확인할 수 있는 부분인데 가설검정으로 불필요한 시간을 소비한것은아닌가??
  -> 가장 고민이 많았던 어려운 점이었습니다. 기초 통계 스터디를 진행하고 있지만, 적용에 있어서 이 검정방법이 최선인지 더 좋은방법은 없는지, 정규성이나 등분산성의 환경에 따른 어떤
    유연함을 가져야하는지 등. 너무 다양한 요소들 속에서 많은 시간을 할애하고 있습니다. 논문을 통해서 사용한 가설검정을 어떻게 사용하였는지 검토해보고 지속적으로 가설검정, 전처리, 모델링등 업데이트를 할 예정입니다.  
- 특성공학 : 어떤 feature를 선택해야 하는가는 모델링에 있어서 중요한 부분이다. 다양한 feature selection 모델이 있지만 맞게 적용하는것을 생각하기 쉽지 않았다. 또한 특정 컬럼에 편향된
  결과를 마주하고 있는 지금 계속해서 학습하며 어떻게 수정할지 고민하고 있다.
- 데이터 셋 분리 타이밍 : 전체 데이터 프레임의 속성을 이용하는 전처리나 스케일링은 데이터셋 분리이후 진행하는 것이 맞지만 간단한 결측값을 채우는 것은 이전에도 가능하다고 생각하고 진행하였다.
- 자연발생 이상치의 경우 삭제가 힘들고 다양한 처리방법론이 있는데 스케일의 변화로 왜도와 같은 형태를 극복한 이후에도 여전히 분산, 최대값등이 높을때 어떻게 대처해야할지 계속해서 고민하고 학습을 이어가고 있다.






